{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Layout Detection - Demo & Visualization\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Model loading and inference\n",
    "2. Prediction visualization\n",
    "3. Comparison between Baseline and GroundingDINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from preprocess import Vocab, UniDSet, find_jsons, read_json\n",
    "from model import build_model\n",
    "\n",
    "# Set random seed\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(ckpt_path: str):\n",
    "    \"\"\"Load model checkpoint.\"\"\"\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    \n",
    "    # Restore vocab\n",
    "    vocab = Vocab()\n",
    "    vocab.itos = ckpt['vocab_itos']\n",
    "    vocab.stoi = {t: i for i, t in enumerate(vocab.itos)}\n",
    "    \n",
    "    # Restore config\n",
    "    config = ckpt.get('config', {})\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model(config.get('model', {}), vocab_size=len(vocab))\n",
    "    model.load_state_dict(ckpt['model_state'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, vocab, config\n",
    "\n",
    "# Example: Load your trained model\n",
    "# model, vocab, config = load_checkpoint('outputs/ckpt/grounding_dino_best.pth')\n",
    "print(\"Model loading function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_bbox(bbox, img_w, img_h):\n",
    "    \"\"\"Convert normalized bbox (cx, cy, w, h) to pixel coordinates (x, y, w, h).\"\"\"\n",
    "    cx, cy, nw, nh = bbox\n",
    "    x = (cx - nw / 2.0) * img_w\n",
    "    y = (cy - nh / 2.0) * img_h\n",
    "    w = nw * img_w\n",
    "    h = nh * img_h\n",
    "    return x, y, w, h\n",
    "\n",
    "\n",
    "def plot_prediction(img_path: str, query_text: str, pred_bbox: List[float], \n",
    "                   gt_bbox: List[float] = None, title: str = \"Prediction\"):\n",
    "    \"\"\"Plot image with predicted and ground truth bboxes.\"\"\"\n",
    "    # Load image\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_w, img_h = img.size\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Plot predicted bbox\n",
    "    if pred_bbox is not None:\n",
    "        px, py, pw, ph = denormalize_bbox(pred_bbox, img_w, img_h)\n",
    "        rect_pred = patches.Rectangle(\n",
    "            (px, py), pw, ph,\n",
    "            linewidth=3, edgecolor='red', facecolor='none',\n",
    "            label='Prediction'\n",
    "        )\n",
    "        ax.add_patch(rect_pred)\n",
    "    \n",
    "    # Plot ground truth bbox\n",
    "    if gt_bbox is not None:\n",
    "        gx, gy, gw, gh = denormalize_bbox(gt_bbox, img_w, img_h)\n",
    "        rect_gt = patches.Rectangle(\n",
    "            (gx, gy), gw, gh,\n",
    "            linewidth=3, edgecolor='green', facecolor='none',\n",
    "            linestyle='--', label='Ground Truth'\n",
    "        )\n",
    "        ax.add_patch(rect_gt)\n",
    "    \n",
    "    ax.set_title(f\"{title}\\nQuery: {query_text}\", fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    ax.legend(loc='upper right', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compare_models(img_path: str, query_text: str, \n",
    "                  pred_baseline: List[float], pred_grounding: List[float],\n",
    "                  gt_bbox: List[float] = None):\n",
    "    \"\"\"Compare predictions from Baseline and GroundingDINO.\"\"\"\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_w, img_h = img.size\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    for ax, pred, title in zip(axes, [pred_baseline, pred_grounding], \n",
    "                               ['Baseline', 'GroundingDINO']):\n",
    "        ax.imshow(img)\n",
    "        \n",
    "        # Predicted bbox\n",
    "        if pred is not None:\n",
    "            px, py, pw, ph = denormalize_bbox(pred, img_w, img_h)\n",
    "            rect_pred = patches.Rectangle(\n",
    "                (px, py), pw, ph,\n",
    "                linewidth=3, edgecolor='red', facecolor='none',\n",
    "                label='Prediction'\n",
    "            )\n",
    "            ax.add_patch(rect_pred)\n",
    "        \n",
    "        # Ground truth bbox\n",
    "        if gt_bbox is not None:\n",
    "            gx, gy, gw, gh = denormalize_bbox(gt_bbox, img_w, img_h)\n",
    "            rect_gt = patches.Rectangle(\n",
    "                (gx, gy), gw, gh,\n",
    "                linewidth=3, edgecolor='green', facecolor='none',\n",
    "                linestyle='--', label='Ground Truth'\n",
    "            )\n",
    "            ax.add_patch(rect_gt)\n",
    "        \n",
    "        ax.set_title(f\"{title}\\nQuery: {query_text}\", fontsize=14, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        ax.legend(loc='upper right', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualization functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_single(model, vocab, img_path: str, query_text: str, img_size: int = 512):\n",
    "    \"\"\"Predict bbox for a single image and query.\"\"\"\n",
    "    from torchvision import transforms as T\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    orig_w, orig_h = img.size\n",
    "    \n",
    "    transform = T.Compose([\n",
    "        T.Resize((img_size, img_size)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)  # (1, 3, H, W)\n",
    "    \n",
    "    # Tokenize query\n",
    "    tokens = vocab.encode(query_text, max_len=40)\n",
    "    tokens_tensor = torch.tensor([tokens], dtype=torch.long).to(device)  # (1, L)\n",
    "    lengths = torch.tensor([len(tokens)], dtype=torch.long).to(device)   # (1,)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    pred = model(img_tensor, tokens_tensor, lengths)  # (1, 4)\n",
    "    pred_bbox = pred[0].cpu().numpy().tolist()  # [cx, cy, w, h] normalized\n",
    "    \n",
    "    return pred_bbox\n",
    "\n",
    "print(\"Inference function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load model and make prediction\n",
    "# Uncomment and modify paths as needed\n",
    "\n",
    "# # Load model\n",
    "# model, vocab, config = load_checkpoint('outputs/ckpt/grounding_dino_best.pth')\n",
    "# img_size = config.get('data', {}).get('img_size', 512)\n",
    "\n",
    "# # Example image and query\n",
    "# img_path = './data/val/jpg/example.jpg'\n",
    "# query_text = '표를 찾아주세요'\n",
    "# gt_bbox = [0.5, 0.5, 0.3, 0.2]  # Ground truth (if available)\n",
    "\n",
    "# # Predict\n",
    "# pred_bbox = predict_single(model, vocab, img_path, query_text, img_size)\n",
    "\n",
    "# # Visualize\n",
    "# plot_prediction(img_path, query_text, pred_bbox, gt_bbox, title=\"GroundingDINO Prediction\")\n",
    "\n",
    "print(\"Example usage template provided.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Visualization from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_samples(model, vocab, json_dir: str, jpg_dir: str, \n",
    "                            num_samples: int = 5, img_size: int = 512):\n",
    "    \"\"\"Visualize random samples from dataset.\"\"\"\n",
    "    # Load dataset\n",
    "    json_files = find_jsons(json_dir)\n",
    "    dataset = UniDSet(json_files, jpg_dir=jpg_dir, vocab=vocab, \n",
    "                     build_vocab=False, resize_to=(img_size, img_size))\n",
    "    \n",
    "    # Get samples with ground truth\n",
    "    valid_indices = [i for i in range(len(dataset)) if dataset[i]['target'] is not None]\n",
    "    \n",
    "    if len(valid_indices) == 0:\n",
    "        print(\"No samples with ground truth found.\")\n",
    "        return\n",
    "    \n",
    "    # Random sample\n",
    "    sample_indices = random.sample(valid_indices, min(num_samples, len(valid_indices)))\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        sample = dataset[idx]\n",
    "        \n",
    "        # Get image path\n",
    "        img_path = dataset.items[idx]['img']\n",
    "        query_text = sample['query_text']\n",
    "        gt_bbox = sample['target'].numpy().tolist() if sample['target'] is not None else None\n",
    "        \n",
    "        # Predict\n",
    "        pred_bbox = predict_single(model, vocab, img_path, query_text, img_size)\n",
    "        \n",
    "        # Calculate IoU if GT available\n",
    "        if gt_bbox is not None:\n",
    "            from test import iou_xywh_pixel\n",
    "            img = Image.open(img_path)\n",
    "            W, H = img.size\n",
    "            \n",
    "            # Convert to pixel coords\n",
    "            px, py, pw, ph = denormalize_bbox(pred_bbox, W, H)\n",
    "            gx, gy, gw, gh = denormalize_bbox(gt_bbox, W, H)\n",
    "            \n",
    "            iou = iou_xywh_pixel([px, py, pw, ph], [gx, gy, gw, gh])\n",
    "            print(f\"\\nSample {idx} - Query: {query_text}\")\n",
    "            print(f\"IoU: {iou:.4f}\")\n",
    "        \n",
    "        # Visualize\n",
    "        plot_prediction(img_path, query_text, pred_bbox, gt_bbox, \n",
    "                       title=f\"Sample {idx}\")\n",
    "\n",
    "# Example usage:\n",
    "# visualize_random_samples(model, vocab, './data/val/json', './data/val/jpg', num_samples=3)\n",
    "\n",
    "print(\"Batch visualization function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison (Baseline vs GroundingDINO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare two models\n",
    "# Uncomment and modify as needed\n",
    "\n",
    "# # Load both models\n",
    "# model_baseline, vocab_baseline, _ = load_checkpoint('outputs/ckpt/baseline_best.pth')\n",
    "# model_grounding, vocab_grounding, _ = load_checkpoint('outputs/ckpt/grounding_dino_best.pth')\n",
    "\n",
    "# img_path = './data/val/jpg/example.jpg'\n",
    "# query_text = '차트를 찾아주세요'\n",
    "# gt_bbox = [0.5, 0.5, 0.3, 0.2]  # Ground truth\n",
    "\n",
    "# # Predict with both models\n",
    "# pred_baseline = predict_single(model_baseline, vocab_baseline, img_path, query_text)\n",
    "# pred_grounding = predict_single(model_grounding, vocab_grounding, img_path, query_text)\n",
    "\n",
    "# # Compare\n",
    "# compare_models(img_path, query_text, pred_baseline, pred_grounding, gt_bbox)\n",
    "\n",
    "print(\"Model comparison template provided.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention Visualization (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_map(model, vocab, img_path: str, query_text: str, img_size: int = 512):\n",
    "    \"\"\"Visualize attention map (for GroundingDINO models).\"\"\"\n",
    "    # This is a placeholder - requires model modification to extract attention weights\n",
    "    print(\"Attention visualization requires model hooks to extract attention weights.\")\n",
    "    print(\"This is left as an exercise for advanced users.\")\n",
    "    \n",
    "    # TODO: \n",
    "    # 1. Register forward hooks on attention layers\n",
    "    # 2. Extract attention weights during forward pass\n",
    "    # 3. Visualize attention weights as heatmap overlaid on image\n",
    "\n",
    "print(\"Attention visualization placeholder defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive widget for Jupyter (requires ipywidgets)\n",
    "try:\n",
    "    from ipywidgets import interact, widgets\n",
    "    from IPython.display import display\n",
    "    \n",
    "    def interactive_demo(model, vocab, json_dir: str, jpg_dir: str, img_size: int = 512):\n",
    "        \"\"\"Interactive demo with dropdown.\"\"\"\n",
    "        json_files = find_jsons(json_dir)\n",
    "        dataset = UniDSet(json_files, jpg_dir=jpg_dir, vocab=vocab, \n",
    "                         build_vocab=False, resize_to=(img_size, img_size))\n",
    "        \n",
    "        valid_indices = [i for i in range(len(dataset)) if dataset[i]['target'] is not None]\n",
    "        \n",
    "        @interact(sample_idx=widgets.Dropdown(options=valid_indices, description='Sample:'))\n",
    "        def show_prediction(sample_idx):\n",
    "            sample = dataset[sample_idx]\n",
    "            img_path = dataset.items[sample_idx]['img']\n",
    "            query_text = sample['query_text']\n",
    "            gt_bbox = sample['target'].numpy().tolist() if sample['target'] is not None else None\n",
    "            \n",
    "            pred_bbox = predict_single(model, vocab, img_path, query_text, img_size)\n",
    "            plot_prediction(img_path, query_text, pred_bbox, gt_bbox, \n",
    "                           title=f\"Sample {sample_idx}\")\n",
    "    \n",
    "    print(\"Interactive demo function defined.\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"ipywidgets not installed. Install with: pip install ipywidgets\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
